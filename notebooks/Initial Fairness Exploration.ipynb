{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../fair_regression')\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from FairLogReg import FairLogisticRegression\n",
    "from DataPreprocessing import get_adult_data\n",
    "\n",
    "\n",
    "def train_split(x, y, pct):\n",
    "    nsamples = len(y)\n",
    "    nsamples_val = int(pct * nsamples)\n",
    "    srs = torch.utils.data.sampler.SubsetRandomSampler(range(nsamples))\n",
    "    idxs = []\n",
    "    for n, i in enumerate(srs):\n",
    "        idxs.append(i)\n",
    "        if n == nsamples_val - 1:\n",
    "            x_val = x[idxs]\n",
    "            y_val = y[idxs]\n",
    "            idxs = []\n",
    "    x = x[idxs]\n",
    "    y = y[idxs]\n",
    "\n",
    "    return x, y, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data as pandas dataframes\n",
    "s_id = ['Sex_Female']\n",
    "s, x_train, y_train, x_test, y_test = get_adult_data(s_id)\n",
    "\n",
    "# # Save the header info before turning into matrices\n",
    "# x_cols = x_train.columns\n",
    "# y_cols = y_train.columns\n",
    "\n",
    "# Convert the dataframes into PyTorch variables and cuda-fy if available\n",
    "x_train = Variable(torch.from_numpy(x_train.as_matrix()))\n",
    "y_train = Variable(torch.from_numpy(y_train.as_matrix()).long())\n",
    "x_test = Variable(torch.from_numpy(x_test.as_matrix()))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x_train = x_train.cuda()\n",
    "    y_train = y_train.cuda()\n",
    "    x_test = x_test.cuda()\n",
    "\n",
    "# We'll only compare y_test as a numpy array, so don't bother to convert\n",
    "y_test = y_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flr = FairLogisticRegression(l_fair=0.06, validate=0.2, print_freq=1, ftol=1e-7,\n",
    "                             penalty_type='individual', minibatch_size=512,\n",
    "                             batch_fairness=True, n_epochs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/256] Training    CE Loss: 0.43556   Accuracy: 0.80341   Penalty: 0.64862\n",
      "              Validation  CE Loss: 0.45542   Accuracy: 0.80467   Penalty: 0.76008\n",
      "Epoch [2/256] Training    CE Loss: 0.42876   Accuracy: 0.8026   Penalty: 0.59651\n",
      "              Validation  CE Loss: 0.44821   Accuracy: 0.80421   Penalty: 0.72125\n",
      "Epoch [3/256] Training    CE Loss: 0.42753   Accuracy: 0.80437   Penalty: 0.61747\n",
      "              Validation  CE Loss: 0.44672   Accuracy: 0.80743   Penalty: 0.74895\n",
      "Epoch [4/256] Training    CE Loss: 0.42744   Accuracy: 0.80537   Penalty: 0.62724\n",
      "              Validation  CE Loss: 0.44627   Accuracy: 0.80805   Penalty: 0.76348\n",
      "Epoch [5/256] Training    CE Loss: 0.4275   Accuracy: 0.80544   Penalty: 0.63122\n",
      "              Validation  CE Loss: 0.4461   Accuracy: 0.80881   Penalty: 0.77033\n",
      "Epoch [6/256] Training    CE Loss: 0.42758   Accuracy: 0.80556   Penalty: 0.63321\n",
      "              Validation  CE Loss: 0.44603   Accuracy: 0.80912   Penalty: 0.77413\n",
      "Epoch [7/256] Training    CE Loss: 0.42766   Accuracy: 0.80564   Penalty: 0.63471\n",
      "              Validation  CE Loss: 0.44601   Accuracy: 0.80928   Penalty: 0.77685\n",
      "Epoch [8/256] Training    CE Loss: 0.42772   Accuracy: 0.80571   Penalty: 0.6361\n",
      "              Validation  CE Loss: 0.446   Accuracy: 0.80974   Penalty: 0.7791\n",
      "Epoch [9/256] Training    CE Loss: 0.42778   Accuracy: 0.80564   Penalty: 0.63742\n",
      "              Validation  CE Loss: 0.446   Accuracy: 0.81004   Penalty: 0.78105\n",
      "Epoch [10/256] Training    CE Loss: 0.42784   Accuracy: 0.80575   Penalty: 0.63866\n",
      "              Validation  CE Loss: 0.446   Accuracy: 0.81004   Penalty: 0.78275\n",
      "Epoch [11/256] Training    CE Loss: 0.42788   Accuracy: 0.80579   Penalty: 0.63982\n",
      "              Validation  CE Loss: 0.446   Accuracy: 0.80989   Penalty: 0.78425\n",
      "Epoch [12/256] Training    CE Loss: 0.42793   Accuracy: 0.80579   Penalty: 0.64087\n",
      "              Validation  CE Loss: 0.44601   Accuracy: 0.81004   Penalty: 0.78557\n",
      "Epoch [13/256] Training    CE Loss: 0.42796   Accuracy: 0.80579   Penalty: 0.64183\n",
      "              Validation  CE Loss: 0.44602   Accuracy: 0.81035   Penalty: 0.78675\n",
      "Epoch [14/256] Training    CE Loss: 0.428   Accuracy: 0.80587   Penalty: 0.64269\n",
      "              Validation  CE Loss: 0.44603   Accuracy: 0.81066   Penalty: 0.78779\n",
      "Epoch [15/256] Training    CE Loss: 0.42804   Accuracy: 0.80629   Penalty: 0.64348\n",
      "              Validation  CE Loss: 0.44603   Accuracy: 0.81066   Penalty: 0.78873\n",
      "Epoch [16/256] Training    CE Loss: 0.42807   Accuracy: 0.80648   Penalty: 0.64419\n",
      "              Validation  CE Loss: 0.44604   Accuracy: 0.81066   Penalty: 0.78957\n",
      "Epoch [17/256] Training    CE Loss: 0.4281   Accuracy: 0.80663   Penalty: 0.64483\n",
      "              Validation  CE Loss: 0.44605   Accuracy: 0.81081   Penalty: 0.79033\n",
      "Epoch [18/256] Training    CE Loss: 0.42813   Accuracy: 0.8069   Penalty: 0.64541\n",
      "              Validation  CE Loss: 0.44606   Accuracy: 0.81066   Penalty: 0.79102\n",
      "Epoch [19/256] Training    CE Loss: 0.42816   Accuracy: 0.80702   Penalty: 0.64594\n",
      "              Validation  CE Loss: 0.44606   Accuracy: 0.81066   Penalty: 0.79166\n",
      "Epoch [20/256] Training    CE Loss: 0.42819   Accuracy: 0.80713   Penalty: 0.64643\n",
      "              Validation  CE Loss: 0.44607   Accuracy: 0.81112   Penalty: 0.79223\n",
      "Epoch [21/256] Training    CE Loss: 0.42822   Accuracy: 0.80717   Penalty: 0.64687\n",
      "              Validation  CE Loss: 0.44608   Accuracy: 0.81143   Penalty: 0.79277\n",
      "Epoch [22/256] Training    CE Loss: 0.42825   Accuracy: 0.80729   Penalty: 0.64728\n",
      "              Validation  CE Loss: 0.44609   Accuracy: 0.81173   Penalty: 0.79326\n",
      "Epoch [23/256] Training    CE Loss: 0.42827   Accuracy: 0.8074   Penalty: 0.64765\n",
      "              Validation  CE Loss: 0.44609   Accuracy: 0.81173   Penalty: 0.79371\n",
      "Epoch [24/256] Training    CE Loss: 0.4283   Accuracy: 0.80748   Penalty: 0.648\n",
      "              Validation  CE Loss: 0.4461   Accuracy: 0.81173   Penalty: 0.79414\n",
      "Epoch [25/256] Training    CE Loss: 0.42832   Accuracy: 0.80767   Penalty: 0.64832\n",
      "              Validation  CE Loss: 0.44611   Accuracy: 0.81189   Penalty: 0.79453\n",
      "Epoch [26/256] Training    CE Loss: 0.42835   Accuracy: 0.80767   Penalty: 0.64862\n",
      "              Validation  CE Loss: 0.44611   Accuracy: 0.81219   Penalty: 0.7949\n",
      "Epoch [27/256] Training    CE Loss: 0.42837   Accuracy: 0.80771   Penalty: 0.6489\n",
      "              Validation  CE Loss: 0.44612   Accuracy: 0.8125   Penalty: 0.79524\n",
      "Epoch [28/256] Training    CE Loss: 0.42839   Accuracy: 0.80779   Penalty: 0.64916\n",
      "              Validation  CE Loss: 0.44613   Accuracy: 0.81265   Penalty: 0.79556\n",
      "Epoch [29/256] Training    CE Loss: 0.42842   Accuracy: 0.80786   Penalty: 0.6494\n",
      "              Validation  CE Loss: 0.44613   Accuracy: 0.81296   Penalty: 0.79587\n",
      "Epoch [30/256] Training    CE Loss: 0.42844   Accuracy: 0.80794   Penalty: 0.64963\n",
      "              Validation  CE Loss: 0.44614   Accuracy: 0.81327   Penalty: 0.79615\n",
      "Epoch [31/256] Training    CE Loss: 0.42845   Accuracy: 0.80805   Penalty: 0.64984\n",
      "              Validation  CE Loss: 0.44614   Accuracy: 0.81311   Penalty: 0.79642\n",
      "Epoch [32/256] Training    CE Loss: 0.42847   Accuracy: 0.80825   Penalty: 0.65004\n",
      "              Validation  CE Loss: 0.44615   Accuracy: 0.81296   Penalty: 0.79668\n",
      "Epoch [33/256] Training    CE Loss: 0.42849   Accuracy: 0.8084   Penalty: 0.65022\n",
      "              Validation  CE Loss: 0.44615   Accuracy: 0.81327   Penalty: 0.79692\n",
      "Epoch [34/256] Training    CE Loss: 0.42851   Accuracy: 0.80844   Penalty: 0.6504\n",
      "              Validation  CE Loss: 0.44616   Accuracy: 0.81342   Penalty: 0.79714\n",
      "Epoch [35/256] Training    CE Loss: 0.42852   Accuracy: 0.80851   Penalty: 0.65056\n",
      "              Validation  CE Loss: 0.44616   Accuracy: 0.81342   Penalty: 0.79736\n",
      "Epoch [36/256] Training    CE Loss: 0.42854   Accuracy: 0.80863   Penalty: 0.65072\n",
      "              Validation  CE Loss: 0.44617   Accuracy: 0.81373   Penalty: 0.79756\n",
      "Epoch [37/256] Training    CE Loss: 0.42855   Accuracy: 0.80871   Penalty: 0.65086\n",
      "              Validation  CE Loss: 0.44617   Accuracy: 0.81388   Penalty: 0.79775\n",
      "Epoch [38/256] Training    CE Loss: 0.42857   Accuracy: 0.80886   Penalty: 0.651\n",
      "              Validation  CE Loss: 0.44618   Accuracy: 0.81388   Penalty: 0.79793\n",
      "Epoch [39/256] Training    CE Loss: 0.42858   Accuracy: 0.8089   Penalty: 0.65113\n",
      "              Validation  CE Loss: 0.44618   Accuracy: 0.81404   Penalty: 0.7981\n",
      "Epoch [40/256] Training    CE Loss: 0.4286   Accuracy: 0.80882   Penalty: 0.65125\n",
      "              Validation  CE Loss: 0.44618   Accuracy: 0.81434   Penalty: 0.79826\n",
      "Epoch [41/256] Training    CE Loss: 0.42861   Accuracy: 0.80882   Penalty: 0.65137\n",
      "              Validation  CE Loss: 0.44619   Accuracy: 0.81434   Penalty: 0.79842\n",
      "Epoch [42/256] Training    CE Loss: 0.42862   Accuracy: 0.80886   Penalty: 0.65148\n",
      "              Validation  CE Loss: 0.44619   Accuracy: 0.81434   Penalty: 0.79856\n",
      "Epoch [43/256] Training    CE Loss: 0.42863   Accuracy: 0.80871   Penalty: 0.65158\n",
      "              Validation  CE Loss: 0.44619   Accuracy: 0.81434   Penalty: 0.7987\n",
      "Epoch [44/256] Training    CE Loss: 0.42864   Accuracy: 0.80871   Penalty: 0.65168\n",
      "              Validation  CE Loss: 0.4462   Accuracy: 0.8145   Penalty: 0.79883\n",
      "Epoch [45/256] Training    CE Loss: 0.42865   Accuracy: 0.80898   Penalty: 0.65177\n",
      "              Validation  CE Loss: 0.4462   Accuracy: 0.81496   Penalty: 0.79895\n",
      "Epoch [46/256] Training    CE Loss: 0.42866   Accuracy: 0.80905   Penalty: 0.65185\n",
      "              Validation  CE Loss: 0.4462   Accuracy: 0.81496   Penalty: 0.79907\n",
      "Epoch [47/256] Training    CE Loss: 0.42867   Accuracy: 0.80913   Penalty: 0.65193\n",
      "              Validation  CE Loss: 0.4462   Accuracy: 0.81496   Penalty: 0.79918\n",
      "Epoch [48/256] Training    CE Loss: 0.42868   Accuracy: 0.80917   Penalty: 0.65201\n",
      "              Validation  CE Loss: 0.44621   Accuracy: 0.81496   Penalty: 0.79928\n",
      "Epoch [49/256] Training    CE Loss: 0.42868   Accuracy: 0.80921   Penalty: 0.65208\n",
      "              Validation  CE Loss: 0.44621   Accuracy: 0.81496   Penalty: 0.79938\n",
      "Epoch [50/256] Training    CE Loss: 0.42869   Accuracy: 0.80924   Penalty: 0.65215\n",
      "              Validation  CE Loss: 0.44621   Accuracy: 0.81496   Penalty: 0.79947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/256] Training    CE Loss: 0.4287   Accuracy: 0.80924   Penalty: 0.65221\n",
      "              Validation  CE Loss: 0.44621   Accuracy: 0.81496   Penalty: 0.79956\n",
      "Epoch [52/256] Training    CE Loss: 0.4287   Accuracy: 0.80928   Penalty: 0.65227\n",
      "              Validation  CE Loss: 0.44621   Accuracy: 0.81496   Penalty: 0.79965\n",
      "Epoch [53/256] Training    CE Loss: 0.42871   Accuracy: 0.80928   Penalty: 0.65233\n",
      "              Validation  CE Loss: 0.44622   Accuracy: 0.81496   Penalty: 0.79973\n",
      "Epoch [54/256] Training    CE Loss: 0.42872   Accuracy: 0.80932   Penalty: 0.65238\n",
      "              Validation  CE Loss: 0.44622   Accuracy: 0.81496   Penalty: 0.7998\n",
      "Epoch [55/256] Training    CE Loss: 0.42872   Accuracy: 0.80932   Penalty: 0.65243\n",
      "              Validation  CE Loss: 0.44622   Accuracy: 0.81496   Penalty: 0.79987\n",
      "Epoch [56/256] Training    CE Loss: 0.42873   Accuracy: 0.80932   Penalty: 0.65248\n",
      "              Validation  CE Loss: 0.44622   Accuracy: 0.81511   Penalty: 0.79994\n",
      "Epoch [57/256] Training    CE Loss: 0.42873   Accuracy: 0.80928   Penalty: 0.65252\n",
      "              Validation  CE Loss: 0.44622   Accuracy: 0.81511   Penalty: 0.8\n",
      "Epoch [58/256] Training    CE Loss: 0.42874   Accuracy: 0.80924   Penalty: 0.65256\n",
      "              Validation  CE Loss: 0.44622   Accuracy: 0.81511   Penalty: 0.80006\n",
      "Epoch [59/256] Training    CE Loss: 0.42874   Accuracy: 0.80932   Penalty: 0.6526\n",
      "              Validation  CE Loss: 0.44623   Accuracy: 0.81511   Penalty: 0.80011\n",
      "Epoch [60/256] Training    CE Loss: 0.42875   Accuracy: 0.80932   Penalty: 0.65264\n",
      "              Validation  CE Loss: 0.44623   Accuracy: 0.81511   Penalty: 0.80017\n",
      "Epoch [61/256] Training    CE Loss: 0.42875   Accuracy: 0.80932   Penalty: 0.65267\n",
      "              Validation  CE Loss: 0.44623   Accuracy: 0.81511   Penalty: 0.80022\n",
      "Epoch [62/256] Training    CE Loss: 0.42875   Accuracy: 0.80936   Penalty: 0.65271\n",
      "              Validation  CE Loss: 0.44623   Accuracy: 0.81511   Penalty: 0.80026\n",
      "Epoch [63/256] Training    CE Loss: 0.42876   Accuracy: 0.80936   Penalty: 0.65274\n",
      "              Validation  CE Loss: 0.44623   Accuracy: 0.81511   Penalty: 0.80031\n",
      "Epoch [64/256] Training    CE Loss: 0.42876   Accuracy: 0.80936   Penalty: 0.65277\n",
      "              Validation  CE Loss: 0.44623   Accuracy: 0.81511   Penalty: 0.80035\n",
      "Epoch [65/256] Training    CE Loss: 0.42876   Accuracy: 0.80936   Penalty: 0.65279\n",
      "              Validation  CE Loss: 0.44623   Accuracy: 0.81511   Penalty: 0.80039\n",
      "Epoch [66/256] Training    CE Loss: 0.42877   Accuracy: 0.8094   Penalty: 0.65282\n",
      "              Validation  CE Loss: 0.44623   Accuracy: 0.81511   Penalty: 0.80042\n",
      "Epoch [67/256] Training    CE Loss: 0.42877   Accuracy: 0.80944   Penalty: 0.65284\n",
      "              Validation  CE Loss: 0.44623   Accuracy: 0.81511   Penalty: 0.80046\n",
      "Epoch [68/256] Training    CE Loss: 0.42877   Accuracy: 0.80944   Penalty: 0.65286\n",
      "              Validation  CE Loss: 0.44623   Accuracy: 0.81496   Penalty: 0.80049\n",
      "Epoch [69/256] Training    CE Loss: 0.42878   Accuracy: 0.80944   Penalty: 0.65288\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81496   Penalty: 0.80052\n",
      "Epoch [70/256] Training    CE Loss: 0.42878   Accuracy: 0.80944   Penalty: 0.6529\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81496   Penalty: 0.80055\n",
      "Epoch [71/256] Training    CE Loss: 0.42878   Accuracy: 0.80944   Penalty: 0.65292\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81496   Penalty: 0.80058\n",
      "Epoch [72/256] Training    CE Loss: 0.42878   Accuracy: 0.80947   Penalty: 0.65294\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81496   Penalty: 0.8006\n",
      "Epoch [73/256] Training    CE Loss: 0.42878   Accuracy: 0.80947   Penalty: 0.65295\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81496   Penalty: 0.80063\n",
      "Epoch [74/256] Training    CE Loss: 0.42879   Accuracy: 0.80947   Penalty: 0.65297\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.8148   Penalty: 0.80065\n",
      "Epoch [75/256] Training    CE Loss: 0.42879   Accuracy: 0.80947   Penalty: 0.65298\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.8148   Penalty: 0.80067\n",
      "Epoch [76/256] Training    CE Loss: 0.42879   Accuracy: 0.80947   Penalty: 0.653\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80069\n",
      "Epoch [77/256] Training    CE Loss: 0.42879   Accuracy: 0.80947   Penalty: 0.65301\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80071\n",
      "Epoch [78/256] Training    CE Loss: 0.42879   Accuracy: 0.80947   Penalty: 0.65302\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80073\n",
      "Epoch [79/256] Training    CE Loss: 0.4288   Accuracy: 0.80947   Penalty: 0.65303\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80074\n",
      "Epoch [80/256] Training    CE Loss: 0.4288   Accuracy: 0.80947   Penalty: 0.65304\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80076\n",
      "Epoch [81/256] Training    CE Loss: 0.4288   Accuracy: 0.80947   Penalty: 0.65305\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80077\n",
      "Epoch [82/256] Training    CE Loss: 0.4288   Accuracy: 0.80947   Penalty: 0.65306\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80079\n",
      "Epoch [83/256] Training    CE Loss: 0.4288   Accuracy: 0.80947   Penalty: 0.65306\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.8008\n",
      "Epoch [84/256] Training    CE Loss: 0.4288   Accuracy: 0.80947   Penalty: 0.65307\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80081\n",
      "Epoch [85/256] Training    CE Loss: 0.4288   Accuracy: 0.80947   Penalty: 0.65308\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80082\n",
      "Epoch [86/256] Training    CE Loss: 0.4288   Accuracy: 0.80947   Penalty: 0.65309\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80083\n",
      "Epoch [87/256] Training    CE Loss: 0.4288   Accuracy: 0.80947   Penalty: 0.65309\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80084\n",
      "Epoch [88/256] Training    CE Loss: 0.42881   Accuracy: 0.80947   Penalty: 0.6531\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80085\n",
      "Epoch [89/256] Training    CE Loss: 0.42881   Accuracy: 0.80947   Penalty: 0.6531\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80086\n",
      "Epoch [90/256] Training    CE Loss: 0.42881   Accuracy: 0.80947   Penalty: 0.65311\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80087\n",
      "Epoch [91/256] Training    CE Loss: 0.42881   Accuracy: 0.80947   Penalty: 0.65311\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80087\n",
      "Epoch [92/256] Training    CE Loss: 0.42881   Accuracy: 0.80947   Penalty: 0.65312\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80088\n",
      "Epoch [93/256] Training    CE Loss: 0.42881   Accuracy: 0.80947   Penalty: 0.65312\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80089\n",
      "Epoch [94/256] Training    CE Loss: 0.42881   Accuracy: 0.80947   Penalty: 0.65312\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80089\n",
      "Epoch [95/256] Training    CE Loss: 0.42881   Accuracy: 0.80947   Penalty: 0.65313\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.8009\n",
      "Epoch [96/256] Training    CE Loss: 0.42881   Accuracy: 0.80951   Penalty: 0.65313\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.8009\n",
      "Epoch [97/256] Training    CE Loss: 0.42881   Accuracy: 0.80951   Penalty: 0.65313\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80091\n",
      "Epoch [98/256] Training    CE Loss: 0.42881   Accuracy: 0.80951   Penalty: 0.65313\n",
      "              Validation  CE Loss: 0.44624   Accuracy: 0.81465   Penalty: 0.80091\n",
      "Epoch [99/256] Training    CE Loss: 0.42881   Accuracy: 0.80951   Penalty: 0.65314\n",
      "              Validation  CE Loss: 0.44625   Accuracy: 0.81465   Penalty: 0.80092\n",
      "Epoch [100/256] Training    CE Loss: 0.42881   Accuracy: 0.80951   Penalty: 0.65314\n",
      "              Validation  CE Loss: 0.44625   Accuracy: 0.81465   Penalty: 0.80092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [101/256] Training    CE Loss: 0.42881   Accuracy: 0.80951   Penalty: 0.65314\n",
      "              Validation  CE Loss: 0.44625   Accuracy: 0.81465   Penalty: 0.80093\n",
      "Epoch [102/256] Training    CE Loss: 0.42881   Accuracy: 0.80951   Penalty: 0.65314\n",
      "              Validation  CE Loss: 0.44625   Accuracy: 0.81465   Penalty: 0.80093\n",
      "Epoch [103/256] Training    CE Loss: 0.42881   Accuracy: 0.80951   Penalty: 0.65314\n",
      "              Validation  CE Loss: 0.44625   Accuracy: 0.81465   Penalty: 0.80093\n",
      "Epoch [104/256] Training    CE Loss: 0.42881   Accuracy: 0.80951   Penalty: 0.65315\n",
      "              Validation  CE Loss: 0.44625   Accuracy: 0.81465   Penalty: 0.80094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<FairLogReg.FairLogisticRegression at 0x114b95be0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flr.fit(x_train, y_train, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = flr.predict(x_test).data.cpu().numpy()\n",
    "y_probas = torch.nn.Softmax(dim=1)(flr.model.forward(x_test)).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1888704625023033"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8889499552639926"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_probas[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x127b9fcf8>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1244ee7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "curve = roc_curve(y_test, y_probas[:, 1])\n",
    "\n",
    "plt.plot(*curve[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class converting_FairLogReg(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs.copy()\n",
    "        self.s = kwargs['s']\n",
    "        del kwargs['s']\n",
    "\n",
    "        self.flr = FairLogisticRegression(**kwargs)\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return self.params.copy()\n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        if 's' in kwargs:\n",
    "            self.s = kwargs['s']\n",
    "        if 'lr' in kwargs:\n",
    "            self.flr.lr = kwargs['lr']\n",
    "        if 'n_classes' in kwargs:\n",
    "            self.flr.n_classes = kwargs['n_classes']\n",
    "        if 'ftol' in kwargs:\n",
    "            self.flr.ftol = kwargs['ftol']\n",
    "        if 'tolerance_grad' in kwargs:\n",
    "            self.flr.tolerance_grad = kwargs['tolerance_grad']\n",
    "        if 'fit_intercept' in kwargs:\n",
    "            self.flr.fit_intercept = kwargs['fit_intercept']\n",
    "        if 'n_epochs' in kwargs:\n",
    "            self.flr.n_epochs = kwargs['n_epochs']\n",
    "        if 'l_fair' in kwargs:\n",
    "            self.flr.l_fair = float(kwargs['l_fair'])\n",
    "        if 'l1' in kwargs:\n",
    "            self.flr.l1 = float(kwargs['l1'])\n",
    "        if 'l2' in kwargs:\n",
    "            self.flr.l2 = float(kwargs['l2'])\n",
    "        if 'minibatch_size' in kwargs:\n",
    "            self.flr.minibatch_size = kwargs['minibatch_size']\n",
    "        if 'njobs' in kwargs:\n",
    "            self.flr.njobs = kwargs['njobs']\n",
    "        if 'validate' in kwargs:\n",
    "            self.flr.validate = kwargs['validate']\n",
    "        if 'print_freq' in kwargs:\n",
    "            self.flr.verbose = kwargs['print_freq']\n",
    "        if 'penalty_type' in kwargs:\n",
    "            self.flr.penalty_type = kwargs['penalty_type']\n",
    "        if 'batch_fairness' in kwargs:\n",
    "            self.flr.mb_fairness = kwargs['batch_fairness']\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_torch(x, y):\n",
    "        x = Variable(torch.from_numpy(x))\n",
    "        y = Variable(torch.from_numpy(y).long())\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        return x, y\n",
    "\n",
    "#     @staticmethod\n",
    "#     def convert_from_torch(self, x, y):\n",
    "#         pass\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        x, y = self.convert_to_torch(x, y)\n",
    "        self.flr.fit(x, y, self.s)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        x, y = self.convert_to_torch(x, y)\n",
    "        return self.flr.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# self, lr=0.01, n_classes=None, ftol=1e-6, tolerance_grad=1e-5,\n",
    "#                  fit_intercept=True, n_epochs=32, l_fair=0.0, l1=0.0, l2=0.0,\n",
    "#                  minibatch_size=32, n_jobs=1, validate=0, print_freq=0,\n",
    "#                  penalty_type='individual', batch_fairness=False\n",
    "\n",
    "ls = np.logspace(-10, 2, 13)\n",
    "indiv = converting_FairLogReg(s=s, n_epochs=256, minibatch_size=512, batch_fairness=True, penalty_type='individual')\n",
    "group = converting_FairLogReg(s=s, n_epochs=256, minibatch_size=512, batch_fairness=True, penalty_type='group')\n",
    "\n",
    "individual_cv = GridSearchCV(indiv, {'l2': ls, 'l_fair': ls}, cv=3)\n",
    "group_cv = GridSearchCV(group, {'l2': ls, 'l_fair': ls}, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=<__main__.converting_FairLogReg object at 0x12895e4a8>,\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'l2': array([1.e-10, 1.e+02]), 'l_fair': array([1.e-10, 1.e+02])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual_cv.fit(x_train.data.cpu().numpy(), y_train.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l2': 1e-10, 'l_fair': 1e-10}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/austin/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/austin/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/austin/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/austin/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/austin/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([145.02807665, 146.73348037,  99.83858387, 132.64187757]),\n",
       " 'mean_score_time': array([0.00258136, 0.00333436, 0.00328048, 0.00363008]),\n",
       " 'mean_test_score': array([0.84874544, 0.75919044, 0.79094621, 0.76591628]),\n",
       " 'mean_train_score': array([0.84891436, 0.7591904 , 0.79113056, 0.76573201]),\n",
       " 'param_l2': masked_array(data=[1e-10, 1e-10, 100.0, 100.0],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_l_fair': masked_array(data=[1e-10, 100.0, 1e-10, 100.0],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'l2': 1e-10, 'l_fair': 1e-10},\n",
       "  {'l2': 1e-10, 'l_fair': 100.0},\n",
       "  {'l2': 100.0, 'l_fair': 1e-10},\n",
       "  {'l2': 100.0, 'l_fair': 100.0}],\n",
       " 'rank_test_score': array([1, 4, 2, 3], dtype=int32),\n",
       " 'split0_test_score': array([0.84586328, 0.76239175, 0.80477244, 0.77307904]),\n",
       " 'split0_train_score': array([0.84995621, 0.7575897 , 0.80347353, 0.76772469]),\n",
       " 'split1_test_score': array([0.84954858, 0.75990421, 0.78376633, 0.76432651]),\n",
       " 'split1_train_score': array([0.84917307, 0.75883353, 0.78343391, 0.76353252]),\n",
       " 'split2_test_score': array([0.85082465, 0.75527501, 0.78429925, 0.76034278]),\n",
       " 'split2_train_score': array([0.84761381, 0.76114798, 0.78648424, 0.76593882]),\n",
       " 'std_fit_time': array([ 3.98420513,  2.9112482 , 39.47815275, 33.43308385]),\n",
       " 'std_score_time': array([0.00045044, 0.00116073, 0.0013922 , 0.00114508]),\n",
       " 'std_test_score': array([0.00210357, 0.00294889, 0.00977926, 0.00531965]),\n",
       " 'std_train_score': array([0.00097362, 0.00147442, 0.00881619, 0.00171768])}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual_cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/austin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.84875, std: 0.00210, params: {'l2': 1e-10, 'l_fair': 1e-10},\n",
       " mean: 0.75919, std: 0.00295, params: {'l2': 1e-10, 'l_fair': 100.0},\n",
       " mean: 0.79095, std: 0.00978, params: {'l2': 100.0, 'l_fair': 1e-10},\n",
       " mean: 0.76592, std: 0.00532, params: {'l2': 100.0, 'l_fair': 100.0}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "add_() received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\n * (float other, float alpha)\n * (Variable other, float alpha)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-93198d706fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Fit them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mplain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mindiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EE380L/final_project/fair_regression/FairLogReg.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, s, writer)\u001b[0m\n\u001b[1;32m    159\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfairness_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_fair\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: add_() received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\n * (float other, float alpha)\n * (Variable other, float alpha)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Import data as pandas dataframes\n",
    "s_id = ['Sex_Female']\n",
    "s, x_train, y_train, x_test, y_test = get_adult_data(s_id)\n",
    "\n",
    "# # Save the header info before turning into matrices\n",
    "x_cols = x_train.columns\n",
    "# y_cols = y_train.columns\n",
    "\n",
    "# Convert the dataframes into PyTorch variables and cuda-fy if available\n",
    "x_train = Variable(torch.from_numpy(x_train.as_matrix()))\n",
    "y_train = Variable(torch.from_numpy(y_train.as_matrix()).long())\n",
    "x_test = Variable(torch.from_numpy(x_test.as_matrix()))\n",
    "y_test = Variable(torch.from_numpy(y_test.as_matrix()).long())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x_train = x_train.cuda()\n",
    "    y_train = y_train.cuda()\n",
    "    x_test = x_test.cuda()\n",
    "    y_test = y_test.cuda()\n",
    "\n",
    "## K-fold Cross Validation for FairLogReg fairness search\n",
    "shared_kwargs = {'ftol': 1e-6, 'n_epochs': 256, 'minibatch_size': 512, 'batch_fairness': True}\n",
    "\n",
    "df_template = {'Type': [], 'MSE': [], 'Score': [], 'Group Penalty': [], 'Individual Penalty': [], 'ID_String': []}\n",
    "penalties = np.logspace(-10, 2, 13)\n",
    "cv = KFold(3)\n",
    "\n",
    "models = {}\n",
    "fold = 0\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for train, test in cv.split(x_train, y_train):\n",
    "    fold += 1\n",
    "    for penalty in penalties:\n",
    "        current_df_dict = df_template.copy()\n",
    "        penalty = float(penalty)  # PyTorch messes up with numpy types\n",
    "\n",
    "        # Define models\n",
    "        plain = FairLogisticRegression(**shared_kwargs)\n",
    "        indiv = FairLogisticRegression(l_fair=penalty, penalty_type='individual', **shared_kwargs)\n",
    "        group = FairLogisticRegression(l_fair=penalty, penalty_type='group', **shared_kwargs)\n",
    "        \n",
    "        # Fit them\n",
    "        plain.fit(x_train, y_train, s)\n",
    "        indiv.fit(x_train, y_train, s)\n",
    "        group.fit(x_train, y_train, s)\n",
    "        \n",
    "        # Save them\n",
    "        plain_str = 'fold: {} pen: {} type: {}'.format(fold, penalty, 'plain')\n",
    "        indiv_str = 'fold: {} pen: {} type: {}'.format(fold, penalty, 'indiv')\n",
    "        group_str = 'fold: {} pen: {} type: {}'.format(fold, penalty, 'group')\n",
    "        \n",
    "        models[plain_str] = plain\n",
    "        models[indiv_str] = indiv\n",
    "        models[group_str] = group\n",
    "        \n",
    "        # Save string for identifying models\n",
    "        current_df_dict['ID_String'].extend([plain_str, indiv_str, group_str])\n",
    "        \n",
    "        # Save score info\n",
    "        plain_score = plain.score(x_test, y_test)\n",
    "        indiv_score = indiv.score(x_test, y_test)\n",
    "        group_score = group.score(x_test, y_test)\n",
    "        current_df_dict['Score'].extend([plain_score, indiv_score, group_score])\n",
    "        \n",
    "        # Save MSE info\n",
    "        plain_pred = plain.predict(x_test).data.cpu().numpy()\n",
    "        indiv_pred = indiv.predict(x_test).data.cpu().numpy()\n",
    "        group_pred = group.predict(x_test).data.cpu().numpy()\n",
    "\n",
    "        plain_mse = mean_squared_error(plain_pred, y_test.data.cpu().numpy())\n",
    "        indiv_mse = mean_squared_error(indiv_pred, y_test.data.cpu().numpy())\n",
    "        group_mse = mean_squared_error(group_pred, y_test.data.cpu().numpy())\n",
    "\n",
    "        current_df_dict['MSE'].extend([plain_mse, indiv_mse, group_mse])\n",
    "        \n",
    "        # Save Penalty info\n",
    "        plain_pen_i = plain.fairness_penalty(x_test, y_test, x_test, y_test, s, penalty_type='individual')\n",
    "        indiv_pen_i = indiv.fairness_penalty(x_test, y_test, x_test, y_test, s, penalty_type='individual')\n",
    "        group_pen_i = group.fairness_penalty(x_test, y_test, x_test, y_test, s, penalty_type='individual')\n",
    "        \n",
    "        plain_pen_g = plain.fairness_penalty(x_test, y_test, x_test, y_test, s, penalty_type='group')\n",
    "        indiv_pen_g = indiv.fairness_penalty(x_test, y_test, x_test, y_test, s, penalty_type='group')\n",
    "        group_pen_g = group.fairness_penalty(x_test, y_test, x_test, y_test, s, penalty_type='group')\n",
    "        \n",
    "        current_df_dict['Individual Penalty'].extend([plain_pen_i, indiv_pen_i, group_pen_i])\n",
    "        current_df_dict['Group Penalty'].extend([plain_pen_g, indiv_pen_g, group_pen_g])\n",
    "        \n",
    "        df = pd.concat([df, pd.DataFrame.from_dict(current_df_dict)])\n",
    "        \n",
    "        df.to_csv('save_temporary_results.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
