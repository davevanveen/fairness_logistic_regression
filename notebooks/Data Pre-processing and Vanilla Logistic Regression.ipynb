{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Vanilla Logistic Regression with L2-Regularizer (or elastic net?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "\n",
    "# Data import routine\n",
    "# 'train_file' and 'test_file' are filenames relative to the notebook\n",
    "# 'feature_list' matches .csv files for the Adult dataset. See usage for exact list.\n",
    "def import_data(train_file, test_file, feature_list):\n",
    "    # Read files\n",
    "    train_data = pd.read_csv(train_file,\n",
    "                             names = feature_list,\n",
    "                             sep = r'\\s*,\\s*',\n",
    "                             engine = 'python',\n",
    "                             na_values = \"?\")\n",
    "    test_data = pd.read_csv(test_file,\n",
    "                            names = feature_list,\n",
    "                            sep = r'\\s*,\\s*',\n",
    "                            engine = 'python',\n",
    "                            na_values = \"?\")\n",
    "    \n",
    "    # Return raw dataframes\n",
    "    return train_data, test_data\n",
    "\n",
    "# Data pre-processing routine\n",
    "# 'raw_data' is the entire data (all features + classes)\n",
    "# 'S_id' should be a valid string with the name of the protected feature\n",
    "# Examples of valid 'S_id': False, 'Sex_Female' - Others TBD\n",
    "# TODO: generalize list of fully dropped features\n",
    "# TODO: generalize list of compressed features\n",
    "# TODO: generalize list of post-encoding dropped features\n",
    "def split_protected_variable(raw_data, S_id):\n",
    "    # Drop completely irrelevant features\n",
    "    # \"fnlwgt\" is a control feature related to how data was sampled\n",
    "    # \"Education\" is the discrete version of \"Education-Num\"\n",
    "    raw_data = raw_data.drop([\"fnlwgt\", \"Education\"], axis = 1)\n",
    "    \n",
    "    # Compress country as US / non-US\n",
    "    raw_data['Country'][raw_data['Country'].str.contains('United-States') == False] = 'Non-US'\n",
    "    # Compress workclass as Private / non-Private\n",
    "    raw_data['Workclass'][raw_data['Workclass'].str.contains('Private') == False] = 'Non-Private'\n",
    "    # Drop relationship\n",
    "    raw_data = raw_data.drop(['Relationship'], axis = 1)\n",
    "    \n",
    "    # Encode categorical features with dummy variables\n",
    "    encoded_data = pd.get_dummies(raw_data,\n",
    "                                  columns = [\"Workclass\", \"Race\", \"Marital Status\",\n",
    "                                             \"Occupation\", \"Sex\",\n",
    "                                             \"Country\", \"Target\"],\n",
    "                                  prefix = [\"Workclass\", \"Race\", \"Marital_status\",\n",
    "                                              \"Occupation\", \"Sex\",\n",
    "                                              \"Country\", \"Target\"])\n",
    "    \n",
    "    # Drop the dummy feature with the most frequent value for colinearity reasons\n",
    "    # This takes care of '?'/NaN by implictly replacing them with the most common entry\n",
    "    encoded_data = encoded_data.drop([\"Workclass_Private\", \"Race_White\",\n",
    "                                      \"Marital_status_Married-civ-spouse\", \"Occupation_Prof-specialty\",\n",
    "                                      \"Sex_Male\", \"Country_United-States\", \"Target_<=50K\"],\n",
    "                                     axis = 1)\n",
    "    \n",
    "    # Centering and normalizing continuous columns entries\n",
    "    numerical_features = [\"Age\", \"Education-Num\", \"Capital Gain\", \"Capital Loss\", \"Hours per week\"]\n",
    "    for featureName in numerical_features:\n",
    "        encoded_data[featureName] = (encoded_data[featureName] - np.mean(encoded_data[featureName], axis=0)) / np.std(encoded_data[featureName], axis=0)\n",
    "        \n",
    "    # Return triple (X, S, y). Return only (X, y) is 'S_id' is void\n",
    "    if not S_id:\n",
    "        X = encoded_data.drop(['Target_>50K'], axis = 1)\n",
    "        S = False\n",
    "        y = encoded_data['Target_>50K']\n",
    "    else:\n",
    "        X = encoded_data.drop([S_id, 'Target_>50K'], axis = 1)\n",
    "        S = encoded_data[S_id]\n",
    "        y = encoded_data['Target_>50K']\n",
    "\n",
    "    return X, S, y\n",
    "\n",
    "# Confusion matrix plot routine\n",
    "# This function prints and plots the confusion matrix\n",
    "# Normalization can be applied by setting `normalize=True`\n",
    "# TODO: Add online reference\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from .csv files\n",
    "train_file = '../data/adult_train.csv'\n",
    "test_file  = '../data/adult_test.csv'\n",
    "feature_list = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital Status\",\n",
    "                \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "                \"Hours per week\", \"Country\", \"Target\"]\n",
    "\n",
    "# Read data\n",
    "train_data, test_data = import_data(train_file, test_file, feature_list)\n",
    "\n",
    "# Comments:\n",
    "# 1. We can compress countries as US / non-US\n",
    "# 2. We can compress race as White / non-White or White / Black / Other\n",
    "# 3. We can compress workclass as Private / non-Private\n",
    "# 3. 'Marital Status' and 'Relationship' appear to have significant overlap\n",
    "# 4. We can guess that 'Occupation' and 'Education' have a big impact on the target\n",
    "# We will use them to pre-determine a gender/race/other bias in the data\n",
    "\n",
    "# Extract X and y - no protected variable\n",
    "S_id = False\n",
    "X_train, S_train, y_train = split_protected_variable(train_data, S_id)\n",
    "X_test, S_test, y_test = split_protected_variable(test_data, S_id)\n",
    "\n",
    "# Extract X, S and y - gender is protected variable\n",
    "#S_id = 'Sex_Female'\n",
    "#X_train, S_train, y_train = split_protected_variable(train_data, S_id)\n",
    "#X_test, S_test, y_test = split_protected_variable(test_data, S_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data visualization\n",
    "\n",
    "# Sanity check for missing values in target column.\n",
    "check_target_nan_train = train_data['Target']\n",
    "check_target_nan_test  = test_data['Target']\n",
    "print('Number of missing entries in target column, train data:', len(check_target_nan_train) - check_target_nan_train.count())\n",
    "print('Number of missing entries in target column, test data:', len(check_target_nan_test) - check_target_nan_test.count())\n",
    "print('If there would have been missing entries in the target column, we\\'d have removed the entire datapoints.')\n",
    "\n",
    "# Plot 1-D histograms for all features\n",
    "fig = plt.figure(figsize = (30, 10))\n",
    "numRows = 3;\n",
    "numCols = 5;\n",
    "\n",
    "for i, col in enumerate(train_data.columns):\n",
    "    ax = fig.add_subplot(numRows, numCols, i+1)\n",
    "    ax.set_title(col)\n",
    "    if train_data.dtypes[col] == np.object:\n",
    "        train_data[col].value_counts().plot(kind = 'bar', axes = ax)\n",
    "        plt.grid()\n",
    "    else:\n",
    "        train_data[col].hist(axes = ax)\n",
    "        \n",
    "plt.subplots_adjust(hspace = 1.2, wspace = 0.3)\n",
    "\n",
    "# Plot cross-histograms of 'Marital Status' and 'Relationship' to ballpark correlation\n",
    "# This is used as justification for dropping 'Relationship'\n",
    "plt.figure(figsize = (30, 10))\n",
    "sns.countplot(x = 'Marital Status', hue = 'Relationship', data = train_data)\n",
    "\n",
    "# Anticipating gender/race bias in the data\n",
    "# Overall bias w.r.t. gender (TODO: race)\n",
    "fig = plt.figure()\n",
    "sns.countplot(x = \"Sex\", hue = \"Target\", data = train_data, palette = \"Blues_d\")\n",
    "\n",
    "# Gender-education bias w.r.t. income\n",
    "male_data = train_data[train_data[\"Sex\"] == 'Male']\n",
    "female_data = train_data[train_data[\"Sex\"] == 'Female']\n",
    "\n",
    "fig = plt.figure()\n",
    "# Male plot\n",
    "fig.add_subplot(2, 1, 1)\n",
    "sns.countplot(x = \"Education-Num\", hue = \"Target\", data = male_data, palette=\"Greens_d\")\n",
    "plt.title(\"Male income vs. education distribution\")\n",
    "# Female plot\n",
    "fig.add_subplot(2, 1, 2)\n",
    "sns.countplot(x = \"Education-Num\", hue = \"Target\", data = female_data, palette=\"Greens_d\")\n",
    "plt.title(\"Female income vs. education distribution\")\n",
    "\n",
    "plt.subplots_adjust(hspace = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run vanilla logistic regression, L2 regularization\n",
    "logreg = LogisticRegressionCV(Cs = list(np.power(10.0, np.arange(-10, 10))),\n",
    "                              penalty = 'l2',\n",
    "                              scoring = 'roc_auc',\n",
    "                              refit = True, # Refit with best C\n",
    "                              cv = 10,\n",
    "                              random_state = 777,\n",
    "                              solver = 'liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Report best C parameter found - TODO!\n",
    "print('Best auc_roc:', logreg.scores_[1].max())\n",
    "\n",
    "# Evaluate test set performance\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "C = confusion_matrix(y_pred, y_test)\n",
    "plot_confusion_matrix(C,\n",
    "                      classes=[\"Income >50k\", \"Income <= 50k\"],\n",
    "                      title='Confusion matrix, l2 regularizer')\n",
    "\n",
    "\n",
    "# Run vanilla logistic regression, Lasso regularization\n",
    "logregL1 = LogisticRegressionCV(Cs = list(np.power(10.0, np.arange(-10, 10))),\n",
    "                                penalty = 'l1',\n",
    "                                scoring = 'roc_auc',\n",
    "                                refit = True, # Refit with best C\n",
    "                                cv = 10,\n",
    "                                random_state = 777,\n",
    "                                solver = 'liblinear')\n",
    "logregL1.fit(X_train, y_train)\n",
    "\n",
    "# Report found coefficients\n",
    "print('Best auc_roc:', logregL1.scores_[1].max())\n",
    "print('Regression coefficients:', logregL1.coef_)\n",
    "\n",
    "# TODO: Need to Lasso a bit more aggressively\n",
    "# Currently at 31 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
